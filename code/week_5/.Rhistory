# TAKE A LOOK AT: https://www.tidytextmining.com/tidytext.html
# load the State of the Union (SOTU) corpus
sotu <- data_corpus_sotu
# a corpus consists of:
# (1) documents: text + doc level data
# (2) corpus metadata
# (3) extras (settings)
head(docvars(sotu))  # document-level variables
meta(sotu)  # corpus-level meta-data
# ndoc identifies the number of documents in a corpus
ndoc(sotu)
# save it as an object to be used later
ndocs <- ndoc(sotu)
# summary of the corpus (provides some summary statistics on the
# text combined with the metadata)
corpusinfo <- summary(sotu, n = ndocs)  # note n default is 100
head(corpusinfo)
tail(corpusinfo)
# does tokens >= types always hold?
# quick visualization
token_plot <- ggplot(data = corpusinfo, aes(x = Date, y = Tokens)) +
geom_line() + geom_point() + theme_bw()
token_plot
# subset corpus  #Biden is not yet in the corpus
summary(corpus_subset(sotu, President == "Trump"))
trump_sotu <- corpus_subset(sotu, President == "Trump")
trump_sotu
# key words in context (KWIC)
kwic_america <- kwic(trump_sotu, pattern = "america",
valuetype = "regex", window = 6)
head(kwic_america)
# keep only the text of the the 2018 SOTU
trump_2018_text <- as.character(trump_sotu)[2]
trump_2018_text
# start with a short character vector
sampletxt <- "The police with their policing strategy instituted a policy?
of general iterations at the Data Science Institute."
# Let's tokenize (break vector into individual words)
tokens <- tokens(sampletxt)
tokens
tokens <- tokens(sampletxt, remove_punct = TRUE)
tokens
names(tokens)
tokens[["text1"]]
# Stemming examples # SnowballC stemmer is based on the Porter stemmer
# (varies by language, english is default)
stems <- tokens_wordstem(tokens)
names(stems)
stems[["text1"]]
tokenized_speech <- tokens(trump_2018_text)
head(unname(unlist(tokenized_speech)), 20)
test <- unlist(tokenized_speech)
test
test <- unname(unlist(tokenized_speech))
test
# alternative using only base R
tokenized_speech <- strsplit(trump_2018_text, " ")
# remove punctuation when tokenizing
tokenized_speech <- tokens(trump_2018_text, remove_punct = TRUE)
head(unname(unlist(tokenized_speech)), 20)
stemmed_speech <- tokens_wordstem(tokenized_speech)
head(unname(unlist(stemmed_speech)), 20)
# Ngrams ---------------------
tokenized_speech_ngrams <- tokens(trump_2018_text, remove_punct = TRUE)
tokenized_speech_ngrams <- tokens_ngrams(tokenized_speech_ngrams,
n = 2L)
head(unname(unlist(tokenized_speech_ngrams)), 20)
tail(unname(unlist(tokenized_speech_ngrams)), 20)
## Types vs. Tokens
ntoken(trump_2018_text)
ntype(trump_2018_text)
tokens(trump_2018_text) %>%
unlist() %>%
unique() %>%
length()
# Creating a DFM
#Arguments passed in the token function can be passed into the DFM
# input can be a document, corpus, etc
trump_2018_dfm <- dfm(trump_2018_text)
# inspect the first few features
trump_2018_dfm[ , 1:10]
# how many rows does this dfm have?
dim(trump_2018_dfm)
# top features in dfm
topfeatures(trump_2018_dfm)
# if you want, can convert to a data frame
out <- convert(dfm(trump_2018_dfm), to = "data.frame")
View(out)
#-----------------------------
# PREPROCESSING (~FEATURE ENGINEERING)
#-----------------------------
# pre-processing can be done prior to dfm OR use the pre-processing
# arguments of dfm
?dfm  # to see all options
# NOTE: lowercase argument is by default TRUE
# punctuation
trump_2018_dfm <- dfm(trump_2018_text, remove_punct = TRUE)
trump_2018_dfm[, 1:10]
# stemming
trump_2018_dfm <- dfm(trump_2018_text, stem = TRUE, remove_punct = TRUE)
trump_2018_dfm[, 1:10]
stopwords("chinese") # from Snowball
trump_2018_dfm_1 <- dfm(trump_2018_text, remove_punct = TRUE)
trump_2018_dfm_2 <- dfm(trump_2018_text, remove = stopwords("english"),
remove_punct = TRUE)
topfeatures(trump_2018_dfm_1)
topfeatures(trump_2018_dfm_2)
install.packages("quanteda.textplots")
# install.packages("quanteda.textplots")
library(quanteda.textplots)
# wordclouds
textplot_wordcloud(trump_2018_dfm_1, max_words = 100)
textplot_wordcloud(trump_2018_dfm_2, max_words = 100)
textplot_wordcloud(trump_2018_dfm_2, max_words = 100)
# WHAT ARE WE WEIGHTING?
# Now we will create a DFM of all the SOTU speeches
full_dfm <- dfm(sotu, remove = stopwords("english"), remove_punct = TRUE)
full_dfm[, 1:5]  # notice sparsity
dim(full_dfm)
topfeatures(full_dfm) # all speeches
topfeatures(full_dfm[nrow(full_dfm),]) # last speech
# tfidf: term frequency-inverse document frequency
# uses the absolute frequency of terms in each document
?dfm_tfidf
weighted_dfm <- dfm_tfidf(full_dfm)
topfeatures(weighted_dfm)
topfeatures(weighted_dfm[nrow(weighted_dfm),])
proportional <- dfm_weight(full_dfm, scheme = "prop")
topfeatures(proportional)
topfeatures(proportional[nrow(proportional),])
# Week 4_updated_code: Updated week 4 code for TaD class
# Author: Noel Johnson (adapted from many others---see below)
# Created: 2-21-2022
# Last Updated: 2-12-2022
# Lab adapted from: Lucia Motolinia, Kevin Munger, Patrick Chester,
# Leslie Huang, Pedro L. Rodriguez, and Lucia Motolinia.
install.packages("stylo")
library(stylo)
# Week 5: week 5 code for TaD class
# Author: Noel Johnson (adapted from many others---see below)
# Created: 3-1-2022
# Last Updated: 2-20-2023
# Lab adapted from: Lucia Motolinia, Kevin Munger, Patrick Chester,
# Leslie Huang, Pedro L. Rodriguez, and Lucia Motolinia.
#----------------------------------------
# Set up environment                   ---
#----------------------------------------
# set path where our data is stored
setwd("/Users/atay508/Documents/George Mason/2022-23 Classes/Text as Data/TaD-Spring-2023/code/week_5")
# load required libraries
library(quanteda)
library(quanteda.corpora)
library(tidyverse)
# read in the files
filenames <- list.files(path = "conservative_manifestos", full.names=TRUE)
filenames
cons_manifestos <- lapply(filenames, readLines)
# because readLines returns a vector with each elements = lines
cons_manifestos <- unlist(lapply(cons_manifestos,
function(x) paste(x, collapse = " ")))
View(cons_manifestos)
# get the date docvar from the filename
dates <- unlist(regmatches(unlist(filenames),
gregexpr("[[:digit:]]+", unlist(filenames))))
# construct tibble (a tibble is an "enhanced" data.frame)
# see ?tibble
manifestos_df <- tibble(year = dates, text = cons_manifestos)
# construct tibble (a tibble is an "enhanced" data.frame)
# see ?tibble
manifestos_df <- tibble(year = dates, text = cons_manifestos)
View(manifestos_df)
#----------------------------------------
# Regular expressions                  ---
#----------------------------------------
# Let us take a step back and have a refresher on grep that we will
# need for later
words <- c("Washington Post", "NYT", "Wall Street Journal",
"Peer-2-Peer", "Red State", "Cheese", "222", ",")
# Exploring by character type
#?grep
# Elements that have alphanumeric characters
grep("\\w", words, value = T)
# Elements that have words that are at least 7 characters long
grep("\\w{7}", words, value = T)
# Elements that contain numbers
grep("\\d", words, value = T)
# Elements that contain non-word characters (Including white space)
grep("\\W", words, value = T)
# note that  grep returns the full element that matched the pattern
words2 <- c("voting", "votes", "devoted", "vote")
# Returns the index of matching items in the vector
grep("^vot", words2)
# Returns the elements of the vector that matched the pattern
grep("^vot", words2, value = T)
# Returns a logical vector indicating whether or not the component
# contains the expression
grepl("^vot", words2)
# you can use the indices to select elements from the original vector
# that you want
words2[grepl("^vot", words2)]
presidents <- c("Roosevelt-33", "Roosevelt-37", "Obama-2003")
# Use gsub to replace patterns with a string
# Parentheses can identify components that can later be referenced by \\1 - \\2
gsub("(\\w+)-(\\d{2})", "\\1-19\\2", presidents)
# We want to use the $ to indicate that the pattern should come at the end
# of the word, to avoid the mismatch in Obama-192003
gsub("(\\w+)-(\\d{2})$", "\\1-19\\2", presidents)
testText <- "The quick brown fox named Seamus jumps over the lazy dog also
named Seamus, with the newspaper from a a boy named Seamus, in his mouth."
# create a dfm
test1 <- dfm(testText)
# view as a dataframe
out <- convert(dfm(testText), to = "data.frame")
View(out)
# keep only words ending in "s"
print(dfm(testText, select = "s$", valuetype = "regex"))
# illustrate how you can access anything you want in the dfm
test2 <- dfm(testText, select = "s$", valuetype = "regex")
test2
testTweets <- c("2 + 2 = 4 #1984",
"I thought you said the park? Why are we at the vet?
#QuestionsFromPets",
"Holy freeway #flooding Batman! #californiastorms taking
their toll.")
# keep only hashtags i.e. expressions starting with a pound sign
print(dfm(testTweets, select="^#", valuetype = "regex"))
# Selecting features from a corpus
data("data_corpus_irishbudgets")
irishbudgets_tokens <- tokens(data_corpus_irishbudgets)
irishbudgets_dfm <- dfm(irishbudgets_tokens)
irishbudgtes_select <- dfm_select(irishbudgets_dfm, pattern=c("tax|budg|^auster"),
valuetype = "regex", selection="keep")
View(irishbudgtes_select)
# You can pass a list of words to the "select" parameter in dfm,
# but using regular expressions can enable you to get all variants of a word
view(irishbudgtes_select[, 1:20])
dim(irishbudgtes_select)
topfeatures(irishbudgtes_select)
library(quanteda.textplots)
# wordclouds
textplot_wordcloud(irishbudgtes_select, max_words = 100)
mytexts <- c("The new law included a capital gains tax,
and an inheritance tax.",
"New York City has raised a taxes:
an income tax and a sales tax.")
mydict <- c("tax", "income", "capital", "gains", "inheritance")
print(dfm(mytexts, select = mydict))
view(dfm(mytexts, select = mydict))
dfm_a <- dfm(mytexts)
df_a <- convert(dfm_a, to = "data.frame")
dfm_b <- dfm(mytexts, select = mydict)
df_b <- convert(dfm_b, to = "data.frame")
# Example: Laver Garry dictionary
# https://rdrr.io/github/kbenoit/quanteda.dictionaries/
# man/data_dictionary_LaverGarry.html
# https://provalisresearch.com/products/content-analysis-software/
# wordstat-dictionary/laver-garry-dictionary-of-policy-position/
# https://github.com/kbenoit/quanteda.dictionaries
# (other dictionaries such as Hu & Liu sentiment are available!)
lgdict <- dictionary(file = "LaverGarry.cat", format = "wordstat")
# What's in this thing?
lgdict
# Run the conservative manifestos through this dictionary
manifestos_lg <- dfm(manifestos_df$text, dictionary = lgdict)
# how does this look
as.matrix(manifestos_lg)[1:5, 1:5]
view(manifestos_lg)
featnames(manifestos_lg)
# plot it
plot(manifestos_df$year,
manifestos_lg[,"VALUES.CONSERVATIVE"],
xlab="Year", ylab="Conservative values", type="b", pch=19)
plot(manifestos_df$year,
manifestos_lg[,"INSTITUTIONS.CONSERVATIVE"]
- manifestos_lg[,"INSTITUTIONS.RADICAL"],
xlab="Year", ylab="Net Conservative Institutions", type="b", pch=19)
# RID Dictionary--Regressive Imagery Dictionary
# https://www.kovcomp.co.uk/wordstat/RID.html (multiple languages available!)
#The English Regressive Imagery Dictionary (RID) is composed of about
# 3200 words and roots assigned to 29 categories of primary process cognition,
# 7 categories of secondary process cognition, and 7 categories of emotions.
rid_dict <- dictionary(file = "RID.cat", format = "wordstat")
rid_dict
data("data_corpus_sotu")
sotus_texts <- texts(data_corpus_sotu)
# Get the docvars from the corpus object
year <- (data_corpus_sotu$Date)
sotu_rid_dfm <- dfm(data_corpus_sotu, dictionary = rid_dict)
view(sotu_rid_dfm)
# Look at the categories
featnames(sotu_rid_dfm)
# Inspect the results graphically
plot(year,
sotu_rid_dfm[,"PRIMARY.REGR_KNOL.NARCISSISM"],
xlab="Year", ylab="Narcissism", type="b", pch=19)
plot(year,
sotu_rid_dfm[,"PRIMARY.ICARIAN_IM.FIRE"] +
sotu_rid_dfm[,"PRIMARY.ICARIAN_IM.ASCEND"] +
sotu_rid_dfm[,"PRIMARY.ICARIAN_IM.DESCENT"] +
sotu_rid_dfm[,"PRIMARY.ICARIAN_IM.DEPTH"] +
sotu_rid_dfm[,"PRIMARY.ICARIAN_IM.HEIGHT"] +
sotu_rid_dfm[,"PRIMARY.ICARIAN_IM.WATER"],
xlab="Year", ylab="Icarian-ness", type="b", pch=19)
# Code from "Text Mining with R"
# Date: 2-21-23
# Last Edited: 2-21-23
# Author: Noel Johnson
#############
# Chapter 1: The tidy text format
#############
library(tidyverse)
library(tidytext)
# The unnest_tokens function
text <- c("Because I could not stop for Death -",
"He kindly stopped for me -",
"The Carriage held but just Ourselves -",
"and Immortality")
text
# make a dataframe
text_df <- tibble(line = 1:4, text = text)
View(text_df)
rm(list = ls())
# Code from "Text Mining with R"
# Date: 2-21-23
# Last Edited: 2-21-23
# Author: Noel Johnson
#############
# Chapter 1: The tidy text format
#############
library(tidyverse)
library(tidytext)
# The unnest_tokens function
text <- c("Because I could not stop for Death -",
"He kindly stopped for me -",
"The Carriage held but just Ourselves -",
"and Immortality")
text
# make a dataframe
text_df <- tibble(line = 1:4, text = text)
# if I didn't specify "line" I would get the variable "line"
# text_df <- tibble(text = text)
text_df
text_df %>%
unnest_tokens(word, text)
library(janeaustenr)
view(austen_books())
# let's add line numbers using mutate and chapter numbers using a regex
original_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text,
regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup()
view(original_books)
tidy_books <- original_books %>%
unnest_tokens(word, text)
tidy_books
text_df %>%
unnest_tokens(word, text)
tidy_books
# we can list the data sets currently available to us
data()
# we want to use the stop_words data set in the tidytext package
data(stop_words)
# we remove stop words using an anti_join
# anti_join() return all rows from x without a match in y
tidy_books <- tidy_books %>%
anti_join(stop_words)
View(tidy_books)
tidy_books %>%
count(word, sort = TRUE)
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word)) +
geom_col() +
labs(y = NULL)
# The gutenbergr package
# provides access to the public domain works from the
# Project Gutenberg collection
# the function gutenberg_download() is used to download one or more works
# from Project Gutenberg by ID
library(gutenbergr)
# download four works by H.G. Wells
hgwells <- gutenberg_download(c(35, 36, 5230, 159))
# tidy the texts
tidy_hgwells <- hgwells %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
# what are the most common words?
tidy_hgwells %>%
count(word, sort = TRUE)
# Now let’s get some well-known works of the Brontë sisters,
# whose lives overlapped with Jane Austen’s but who wrote in a different style.
bronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))
tidy_bronte <- bronte %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
# what are the most common words?
tidy_bronte %>%
count(word, sort = TRUE)
frequency <- bind_rows(mutate(tidy_bronte, author = "Brontë Sisters"),
mutate(tidy_hgwells, author = "H.G. Wells"),
mutate(tidy_books, author = "Jane Austen")) %>%
mutate(word = str_extract(word, "[a-z']+")) %>%
count(author, word) %>%
group_by(author) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
pivot_wider(names_from = author, values_from = proportion) %>%
pivot_longer(`Brontë Sisters`:`H.G. Wells`,
names_to = "author", values_to = "proportion")
frequency
# let's plot
library(scales)
ggplot(frequency, aes(x = proportion, y = `Jane Austen`,
color = abs(`Jane Austen` - proportion))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001),
low = "darkslategray4", high = "gray75") +
facet_wrap(~author, ncol = 2) +
theme(legend.position="none") +
labs(y = "Jane Austen", x = NULL)
# correlations
cor.test(data = frequency[frequency$author == "Brontë Sisters",],
~ proportion + `Jane Austen`)
cor.test(data = frequency[frequency$author == "H.G. Wells",],
~ proportion + `Jane Austen`)
# clear the environment
rm(list = ls())
# library textdata
library(tidytext)
library(textdata)
get_sentiments("bing")
library(janeaustenr)
library(tidyverse)
library(stringr)
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(
linenumber = row_number(),
chapter = cumsum(str_detect(text,
regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
bing_positive <- get_sentiments("bing") %>%
filter(sentiment == "positive")
tidy_books %>%
filter(book == "Emma") %>%
inner_join(bing_positive) %>%
count(word, sort = TRUE)
library(tidyr)
jane_austen_sentiment <- tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(book, index = linenumber %/% 80, sentiment) %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
mutate(sentiment = positive - negative)
library(ggplot2)
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book, ncol = 2, scales = "free_x")
bing_word_counts <- tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
bing_word_counts
bing_word_counts %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(x = "Contribution to sentiment",
y = NULL)
custom_stop_words <- bind_rows(tibble(word = c("miss"),
lexicon = c("custom")),
stop_words)
custom_stop_words
p_and_p_sentences <- tibble(text = prideprejudice) %>%
unnest_tokens(sentence, text, token = "sentences")
p_and_p_sentences$sentence[8]
austen_chapters <- austen_books() %>%
group_by(book) %>%
unnest_tokens(chapter, text, token = "regex",
pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
ungroup()
austen_chapters %>%
group_by(book) %>%
summarise(chapters = n())
bingnegative <- get_sentiments("bing") %>%
filter(sentiment == "negative")
wordcounts <- tidy_books %>%
group_by(book, chapter) %>%
summarize(words = n())
tidy_books %>%
semi_join(bingnegative) %>%
group_by(book, chapter) %>%
summarize(negativewords = n()) %>%
left_join(wordcounts, by = c("book", "chapter")) %>%
mutate(ratio = negativewords/words) %>%
filter(chapter != 0) %>%
slice_max(ratio, n = 1) %>%
ungroup()
