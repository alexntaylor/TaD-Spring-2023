install.packages("usethis")
library(pacman)
install.packages("pacman")
library(pacman)
p_load(usethis)
p_load(usethis, tidyverse)
use_git_config(user.name = "Alex Taylor", user.email = "ataylo46@gmu.edu")
usethis::git_default_branch_configure()
usethis::create_github_token()
# Set
gitcreds::gitcreds_set()
load("/Users/atay508/Dropbox/USTC Data/Data Files/Clean_USTC.Rda")
View(Raw_USTC)
unique(Raw_USTC$subject)
library(tidyverse)
Raw_USTC %>%
filter(subject == "Travel, topography, maps and navigational manuals")
Raw_USTC %>%
filter(subject == "Travel, topography, maps and navigational manuals") -> travel_bks
save(travel_bks, file = "~/Downloads/travel_bks.Rda")
library(pacman)
p_load(tidyverse, ggplot2, skimr)
load(file = "~/Dropbox/USTC Data/Data Files/Clean_USTC.Rda")
View(Raw_USTC)
load("/Users/atay508/Documents/George Mason/Papers/Printing Press/Data/ustc_vernac_4-23-22.Rda")
View(ustc_vernac_4_23_22)
ustc_vernac_4_23_22 %>%
filter(place == "antwerp") -> antwerp
ustc_vernac_4_23_22 %>%
filter(place == "amsterdam") -> amsterdam
ggplot() +
geom_line(data = antwerp, stat = "count", aes(x=year), color = "blue") +
geom_line(data = amsterdam, stat = "count", aes(x=year), color = "red")
# Week 2 introducing_quanteda for TaD class
# Author: Noel Johnson (adapted from many others---see below)
# Created: 1-30-2022
# Last Updated: 2-3-2022
# Lab adapted from: Lucia Motolinia, Kevin Munger, Patrick Chester,
# Leslie Huang, Pedro L. Rodriguez, and Lucia Motolinia.
#-----------------------------
# SETTING UP
#-----------------------------
## Set up workspace
# Clear Global Environment
rm(list = ls())
# allow for exact replication
set.seed(100)
## Install quanteda
# Install the latest stable version of quanteda from CRAN
# run this if you don't have quanteda already installed
# install.packages("quanteda")
library(quanteda)
library(tidyverse)
# Devtools and the quanteda corpus
# Install the package "devtools" which is used to install packages
# directly from Github
# install.packages("devtools")
library("devtools")
# Use devtools to install some sample data
devtools::install_github("quanteda/quanteda.corpora")
# Load it into our environment
library(quanteda.corpora)
# load the State of the Union (SOTU) corpus
sotu <- data_corpus_sotu
sotu
# a corpus consists of:
# (1) documents: text + doc level data
# (2) corpus metadata
# (3) extras (settings)
head(docvars(sotu))  # document-level variables
docvars(sotu)
meta(sotu)  # corpus-level meta-data
# ndoc identifies the number of documents in a corpus
ndoc(sotu)
# save it as an object to be used later
ndocs <- ndoc(sotu)
# summary of the corpus (provides some summary statistics on the
# text combined with the metadata)
corpusinfo <- summary(sotu, n = ndocs)  # note n default is 100
View(corpusinfo)
head(corpusinfo)
tail(corpusinfo)
# quick visualization
token_plot <- ggplot(data = corpusinfo, aes(x = Date, y = Tokens)) +
geom_line() + geom_point() + theme_bw()
token_plot
# subset corpus  #Biden is not yet in the corpus
summary(corpus_subset(sotu, President == "Trump"))
trump_sotu <- corpus_subset(sotu, President == "Trump")
trump_sotu
# key words in context (KWIC)
kwic_america <- kwic(trump_sotu, pattern = "america",
valuetype = "regex", window = 6)
head(kwic_america)
# keep only the text of the the 2018 SOTU
trump_2018_text <- as.character(trump_sotu)[2]
trump_2018_text
# start with a short character vector
sampletxt <- "The police with their policing strategy instituted a policy?
of general iterations at the Data Science Institute."
# Week 2 introducing_quanteda for TaD class
# Author: Noel Johnson (adapted from many others---see below)
# Created: 1-30-2022
# Last Updated: 2-3-2022
# Lab adapted from: Lucia Motolinia, Kevin Munger, Patrick Chester,
# Leslie Huang, Pedro L. Rodriguez, and Lucia Motolinia.
#-----------------------------
# SETTING UP
#-----------------------------
## Set up workspace
# Clear Global Environment
rm(list = ls())
# allow for exact replication
set.seed(100)
## Install quanteda
# Install the latest stable version of quanteda from CRAN
# run this if you don't have quanteda already installed
# install.packages("quanteda")
library(quanteda)
library(tidyverse)
# Devtools and the quanteda corpus
# Install the package "devtools" which is used to install packages
# directly from Github
# install.packages("devtools")
library("devtools")
# Use devtools to install some sample data
devtools::install_github("quanteda/quanteda.corpora")
# Load it into our environment
library(quanteda.corpora)
# Read about the data available: https://github.com/quanteda/quanteda.corpora
# Note: Like many R packages, quanteda is still under development so it is
#always changing!
# New features are being added but sometimes functions or function
# parameters are deprecated or renamed.
# to check version
packageVersion("quanteda")
# How would you get an older version of quanteda?
# (For example, if you accidentally installed the dev version from
# GitHub but you want to go back to the last stable release, or you
# want a legacy version to support old code.)
# - Check the CRAN archive
# use the install_version function, e.g.:
# devtools::install_version("quanteda", version = "0.99.12",
# repos = "http://cran.us.r-project.org")
# If you want the latest dev version of quanteda, it's on GitHub,
# devtools::install_github("quanteda/quanteda")
# But we will use the latest version from CRAN for stability/sanity reasons
# Concept review:
# token
# type
# feature
# term
#-----------------------------
# THE CORPUS OBJECT
#-----------------------------
# quanteda's main input object is called a "corpus"
# (a way of organizing text data: generally includes text + metadata)
# THERE ARE OTHER WAYS to organize text data
# TAKE A LOOK AT: https://www.tidytextmining.com/tidytext.html
# load the State of the Union (SOTU) corpus
sotu <- data_corpus_sotu
# a corpus consists of:
# (1) documents: text + doc level data
# (2) corpus metadata
# (3) extras (settings)
head(docvars(sotu))  # document-level variables
meta(sotu)  # corpus-level meta-data
# ndoc identifies the number of documents in a corpus
ndoc(sotu)
# save it as an object to be used later
ndocs <- ndoc(sotu)
# summary of the corpus (provides some summary statistics on the
# text combined with the metadata)
corpusinfo <- summary(sotu, n = ndocs)  # note n default is 100
head(corpusinfo)
tail(corpusinfo)
# does tokens >= types always hold?
# quick visualization
token_plot <- ggplot(data = corpusinfo, aes(x = Date, y = Tokens)) +
geom_line() + geom_point() + theme_bw()
token_plot
# subset corpus  #Biden is not yet in the corpus
summary(corpus_subset(sotu, President == "Trump"))
trump_sotu <- corpus_subset(sotu, President == "Trump")
trump_sotu
# key words in context (KWIC)
kwic_america <- kwic(trump_sotu, pattern = "america",
valuetype = "regex", window = 6)
head(kwic_america)
# keep only the text of the the 2018 SOTU
trump_2018_text <- as.character(trump_sotu)[2]
trump_2018_text
# start with a short character vector
sampletxt <- "The police with their policing strategy instituted a policy?
of general iterations at the Data Science Institute."
# Let's tokenize (break vector into individual words)
tokens <- tokens(sampletxt)
tokens
tokens <- tokens(sampletxt, remove_punct = TRUE)
tokens
names(tokens)
tokens[["text1"]]
# Stemming examples # SnowballC stemmer is based on the Porter stemmer
# (varies by language, english is default)
stems <- tokens_wordstem(tokens)
names(stems)
stems[["text1"]]
tokenized_speech <- tokens(trump_2018_text)
head(unname(unlist(tokenized_speech)), 20)
test <- unlist(tokenized_speech)
test
test <- unname(unlist(tokenized_speech))
test
# alternative using only base R
tokenized_speech <- strsplit(trump_2018_text, " ")
# remove punctuation when tokenizing
tokenized_speech <- tokens(trump_2018_text, remove_punct = TRUE)
head(unname(unlist(tokenized_speech)), 20)
stemmed_speech <- tokens_wordstem(tokenized_speech)
head(unname(unlist(stemmed_speech)), 20)
# Ngrams ---------------------
tokenized_speech_ngrams <- tokens(trump_2018_text, remove_punct = TRUE)
tokenized_speech_ngrams <- tokens_ngrams(tokenized_speech_ngrams,
n = 2L)
head(unname(unlist(tokenized_speech_ngrams)), 20)
tail(unname(unlist(tokenized_speech_ngrams)), 20)
## Types vs. Tokens
ntoken(trump_2018_text)
ntype(trump_2018_text)
tokens(trump_2018_text) %>%
unlist() %>%
unique() %>%
length()
# Creating a DFM
#Arguments passed in the token function can be passed into the DFM
# input can be a document, corpus, etc
trump_2018_dfm <- dfm(trump_2018_text)
# inspect the first few features
trump_2018_dfm[ , 1:10]
# how many rows does this dfm have?
dim(trump_2018_dfm)
# top features in dfm
topfeatures(trump_2018_dfm)
# if you want, can convert to a data frame
out <- convert(dfm(trump_2018_dfm), to = "data.frame")
View(out)
#-----------------------------
# PREPROCESSING (~FEATURE ENGINEERING)
#-----------------------------
# pre-processing can be done prior to dfm OR use the pre-processing
# arguments of dfm
?dfm  # to see all options
# NOTE: lowercase argument is by default TRUE
# punctuation
trump_2018_dfm <- dfm(trump_2018_text, remove_punct = TRUE)
trump_2018_dfm[, 1:10]
# stemming
trump_2018_dfm <- dfm(trump_2018_text, stem = TRUE, remove_punct = TRUE)
trump_2018_dfm[, 1:10]
stopwords("chinese") # from Snowball
trump_2018_dfm_1 <- dfm(trump_2018_text, remove_punct = TRUE)
trump_2018_dfm_2 <- dfm(trump_2018_text, remove = stopwords("english"),
remove_punct = TRUE)
topfeatures(trump_2018_dfm_1)
topfeatures(trump_2018_dfm_2)
install.packages("quanteda.textplots")
# install.packages("quanteda.textplots")
library(quanteda.textplots)
# wordclouds
textplot_wordcloud(trump_2018_dfm_1, max_words = 100)
textplot_wordcloud(trump_2018_dfm_2, max_words = 100)
textplot_wordcloud(trump_2018_dfm_2, max_words = 100)
# WHAT ARE WE WEIGHTING?
# Now we will create a DFM of all the SOTU speeches
full_dfm <- dfm(sotu, remove = stopwords("english"), remove_punct = TRUE)
full_dfm[, 1:5]  # notice sparsity
dim(full_dfm)
topfeatures(full_dfm) # all speeches
topfeatures(full_dfm[nrow(full_dfm),]) # last speech
# tfidf: term frequency-inverse document frequency
# uses the absolute frequency of terms in each document
?dfm_tfidf
weighted_dfm <- dfm_tfidf(full_dfm)
topfeatures(weighted_dfm)
topfeatures(weighted_dfm[nrow(weighted_dfm),])
proportional <- dfm_weight(full_dfm, scheme = "prop")
topfeatures(proportional)
topfeatures(proportional[nrow(proportional),])
# Week 4_updated_code: Updated week 4 code for TaD class
# Author: Noel Johnson (adapted from many others---see below)
# Created: 2-21-2022
# Last Updated: 2-12-2022
# Lab adapted from: Lucia Motolinia, Kevin Munger, Patrick Chester,
# Leslie Huang, Pedro L. Rodriguez, and Lucia Motolinia.
install.packages("stylo")
library(stylo)
#----------------------------------------
# Set up environment                  ---
#----------------------------------------
# clear global environment
rm(list = ls())
# set path where our data is stored
setwd("/Users/atay508/Documents/George Mason/2022-23 Classes/Text as Data/TaD-Spring-2023/code/week_6")
# load required libraries
library(quanteda)
library(quanteda.corpora)
library(readtext)
library(tidyverse)
library(tidyverse)
rm(list = ls())
# set path where our data is stored
setwd("/Users/atay508/Documents/George Mason/2022-23 Classes/Text as Data/TaD-Spring-2023/code/week_6")
# load required libraries
library(quanteda)
library(quanteda.corpora)
library(readtext)
library(tidyverse)
library(quanteda.textmodels)
# load data
news_data <- readRDS("news_data.rds")
# look at the data structure
glimpse(news_data)
# what are the categories in the category variable?
categories <- news_data %>%
filter(!is.na(category)) %>%
count(category, sort = TRUE)
# subset data to only crime and sports and keep relevant variables text
# and class
#(text=headline, class=category)
news_samp <- news_data %>%
filter(category %in% c("CRIME", "SPORTS")) %>%
select(headline, category) %>%
setNames(c("text", "class"))
View(news_samp)
# get a sense of how the text looks
dim(news_samp)
head(news_samp$text[news_samp$class == "CRIME"])
head(news_samp$text[news_samp$class == "SPORTS"])
# some pre-processing (the rest we will let dfm do)
# replace apostrophes
news_samp$text <- gsub(pattern = "'", "", news_samp$text)
head(news_samp$text[news_samp$class == "SPORTS"])
# what's the distribution of classes?
prop.table(table(news_samp$class))
# split sample into training & test sets
set.seed(1984L)
# we will use 80% of the data as our training set
prop_train <- 0.8
ids <- 1:nrow(news_samp)
ids_train <- sample(ids, ceiling(prop_train*length(ids)), replace = FALSE)
ids_test <- ids[-ids_train]
train_set <- news_samp[ids_train,]
test_set <- news_samp[ids_test,]
# get dfm for each set
train_dfm <- dfm(train_set$text, stem = TRUE, remove_punct = TRUE,
remove = stopwords("english"))
# take a look at the dfm
train_dfm[, 1:10]
test_dfm <- dfm(test_set$text, stem = TRUE, remove_punct = TRUE,
remove = stopwords("english"))
# take a look at the dfm
test_dfm[, 1:10]
# another way to take a look?
head(test_dfm)
head(train_dfm)
# match test set dfm to train set dfm features
#?dfm_match
test_dfm <- dfm_match(test_dfm, features = featnames(train_dfm))
# train model on the training set
nb_model <- textmodel_nb(train_dfm, train_set$class,
smooth = 0, prior = "uniform")
# evaluate on test set
predicted_class <- predict(nb_model, newdata = test_dfm)
predicted_class[1:10]
length(predicted_class)
# baseline
baseline_acc <- max(prop.table(table(test_set$class)))
baseline_acc
# get confusion matrix (see slide from class)
cmat <- table(test_set$class, predicted_class)
cmat
# accuracy = (TP + TN) / (TP + FP + TN + FN)
nb_acc <- sum(diag(cmat))/sum(cmat)
# recall = TP / (TP + FN)
nb_recall <- cmat[2,2]/sum(cmat[2,])
# precision = TP / (TP + FP)
nb_precision <- cmat[2,2]/sum(cmat[,2])
nb_f1 <- 2*(nb_recall*nb_precision)/(nb_recall + nb_precision)
# print using the concatenation command "cat"
cat(
"\n",
"Baseline Accuracy: ", baseline_acc, "\n",
"Accuracy:",  nb_acc, "\n",
"Recall:",  nb_recall, "\n",
"Precision:",  nb_precision, "\n",
"F1-score:", nb_f1
)
# train model on the training set using Laplace smoothing
nb_model_sm <- textmodel_nb(train_dfm, train_set$class,
smooth = 1, prior = "uniform")
# evaluate on test set
# don't know why we need "force=TRUE" here.... Works either way.
predicted_class_sm <- predict(nb_model_sm, newdata = test_dfm, force=TRUE)
# get confusion matrix
cmat_sm <- table(test_set$class, predicted_class_sm)
# accuracy = (TP + TN) / (TP + FP + TN + FN)
nb_acc_sm <- sum(diag(cmat_sm))/sum(cmat_sm)
# recall = TP / (TP + FN)
nb_recall_sm <- cmat_sm[2,2]/sum(cmat_sm[2,])
# precision = TP / (TP + FP)
nb_precision_sm <- cmat_sm[2,2]/sum(cmat_sm[,2])
nb_f1_sm <- 2*(nb_recall_sm*nb_precision_sm)/(nb_recall_sm + nb_precision_sm)
# print
cat(
"\n",
"Baseline Accuracy: ", baseline_acc, "\n",
"Accuracy:",  nb_acc_sm, "\n",
"Recall:",  nb_recall_sm, "\n",
"Precision:",  nb_precision_sm, "\n",
"F1-score:", nb_f1_sm
)
# take a look at the most discriminant features (do they pass the smell test)
posterior <- tibble(feature = rownames(t(nb_model_sm$param)),
post_CRIME = t(nb_model_sm$param)[,1],
post_SPORTS = t(nb_model_sm$param)[,2])
posterior %>% arrange(-post_SPORTS) %>% head(10)
posterior %>% arrange(-post_CRIME) %>% head(10)
# what does smoothing do? Reduces the "weight" placed on new
# information (the likelihood) vis-a-vis the prior.
plot(nb_model$param[1,], nb_model_sm$param[1,],
xlim = c(0,0.02), ylim = c(0,0.02), xlab="No Smooth", ylab="Smooth") +
abline(a = 0, b = 1, col = "red")
# Read in conservative and labour manifestos
filenames <- list.files(path = "cons_labour_manifestos")
# Party name and year are in the filename -- we can use regex to
# extract these to use as our docvars
party <- unlist(regmatches(unlist(filenames),
gregexpr("^[[:alpha:]]{3}", unlist(filenames))))
year <- unlist(regmatches(unlist(filenames),
gregexpr("[[:digit:]]+", unlist(filenames))))
# Make a corpus with docvars from this data
cons_labour_manifestos <- corpus(readtext("cons_labour_manifestos/*.txt"))
docvars(cons_labour_manifestos, field = c("party", "year")) <-
data.frame(cbind(party, year))
# But we're going to use a dataframe (...actually a tibble)
cons_labour_df <- tibble(text = texts(cons_labour_manifestos),
class = party,
year = as.integer(year))
View(cons_labour_df)
# Let's take a look...
glimpse(cons_labour_df)
colnames(cons_labour_df)
# what's the class distribution?
prop.table(table(cons_labour_df$class))
# randomly sample a test speech
set.seed(1984L)
ids <- 1:nrow(cons_labour_df)
#we are just excluding one for the test set
ids_test <- sample(ids, 1, replace = FALSE)
ids_train <- ids[-ids_test]
train_set <- cons_labour_df[ids_train,]
test_set <- cons_labour_df[ids_test,]
# create DFMs
train_dfm <- dfm(train_set$text, remove_punct = TRUE,
remove = stopwords("english"))
test_dfm <- dfm(test_set$text, remove_punct = TRUE,
remove = stopwords("english"))
# Word Score model w/o smoothing ----------------
# Y variable must be coded on a binary x in {-1,1} scale,
# so -1 = Conservative and 1 = Labour
# ?textmodel_wordscores
ws_base <- textmodel_wordscores(train_dfm,
y = (2 * as.numeric(train_set$class == "Lab")) - 1
)
train_set$class
train_set$class == "Lab"
as.numeric(train_set$class == "Lab")
(2 * as.numeric(train_set$class == "Lab"))
(2 * as.numeric(train_set$class == "Lab")) - 1
# Look at strongest features
# for labor
lab_features <- sort(ws_base$wordscores, decreasing = TRUE)
lab_features[1:10]
# for conservative
con_features <- sort(ws_base$wordscores, decreasing = FALSE)
con_features[1:10]
# Can also check the score for specific features
ws_base$wordscores[c("drugs", "minorities", "unemployment")]
# predict that test speech
test_set$class
predict(ws_base, newdata = test_dfm)
# Word Score model w smoothing ----------------
# ?textmodel_wordscores
# Y variable must be coded on a binary x in {-1,1} scale, so
# -1 = Conservative and 1 = Labour
ws_sm <- textmodel_wordscores(train_dfm,
y = (2 * as.numeric(train_set$class == "Lab")) - 1,
smooth = 1
)
# Look at strongest features
# for labor
lab_features_sm <- sort(ws_sm$wordscores, decreasing = TRUE)
lab_features_sm[1:10]
# for conservative
con_features_sm <- sort(ws_sm$wordscores, decreasing = FALSE)
con_features_sm[1:10]
# predict that last speech
test_set$class
predict(ws_base, newdata = test_dfm)
# Smoothing
plot(ws_base$wordscores, ws_sm$wordscores, xlim=c(-1, 1), ylim=c(-1, 1),
xlab="No Smooth", ylab="Smooth")
