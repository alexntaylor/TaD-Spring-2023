Lab adapted from: Leslie Huang, Pedro L. Rodriguez and Lucia Motolinia
# Set up workspace
rm(list = ls())
# Loading packages
#install.packages("lsa")
#install.packages("factoextra")
library(quanteda)
library(quanteda.corpora)
library(dplyr)
library(factoextra) # makes it easy to work with PCA (great for visualization)
library(text2vec)
library(lsa)
#set directory
setwd("D:/Users/Lucia/Dropbox/NYU/Classes/TAD 2021/Lab Lectures/W9_04_01_21")
###########
## 1 PCA ##
###########
# 1.2 Example
data("data_corpus_sotu")
SOTU <- corpus_subset(data_corpus_sotu, Date > "1900-01-01")
SOTU_dfm <- dfm(SOTU,
stem = T,
remove_punct = T,
remove = stopwords("english")
)
SOTU_mat <- convert(SOTU_dfm, to = "matrix") # convert to matrix
# run pca
SOTU_pca <- prcomp(SOTU_mat, center = TRUE, scale = TRUE)
# visualize eigenvalues (scree plot: shows percentage of variance explained by each dimension)
fviz_eig(SOTU_pca, addlabels = TRUE)
#how many relevant dimensions are there?
# Loadings for each variable: columns contain the eigenvectors
SOTU_pca$rotation[1:10, 1:5]
dim(SOTU_pca$rotation)
# Can we interpret the dimensions?
pc_loadings <- SOTU_pca$rotation
View(pc_loadings)
# top loadings on PC1
# token loadings
N <- 10
pc1_loading <- tibble(token = rownames(pc_loadings), loading = as.vector(pc_loadings[,1])) %>% arrange(-loading)
pc1_loading$loading <- scale(pc1_loading$loading, center = TRUE)
pc1_loading <- rbind(top_n(pc1_loading, N, loading),top_n(pc1_loading, -N, loading))#lets get the top and bottom 10
pc1_loading <- transform(pc1_loading, token = factor(token, levels = unique(token)))
# plot top tokens according to absolute loading values
ggplot(pc1_loading, aes(token, loading)) +
geom_bar(stat = "identity", fill = ifelse(pc1_loading$loading <= 0, "grey20", "grey70")) +
coord_flip() +
xlab("Tokens") + ylab("Tokens with Top Loadings on PC1") +
scale_colour_grey(start = .3, end = .7) +
theme(panel.background = element_blank())
# Value of the rotated data: your "new", dimensionality reduced data
View(SOTU_pca$x)  # each observation
# function computes cosine similarity between query and all documents and returns N most similar
nearest_neighbors <- function(query, low_dim_space, N = 5, norm = "l2"){
cos_sim <- sim2(x = low_dim_space, y = low_dim_space[query, , drop = FALSE], method = "cosine", norm = norm)
nn <- cos_sim <- cos_sim[order(-cos_sim),]
return(names(nn)[2:(N + 1)])  # query is always the nearest neighbor hence dropped
}
# apply to document retrieval
nearest_neighbors(query = "Obama-2009", low_dim_space = SOTU_pca$x, N = 5, norm = "l2")
# Let's keep using the SOTU data from before
SOTU_mat_lsa <- convert(SOTU_dfm, to = "lsa") # convert to transposed matrix
#(so terms are rows and columns are documents = TDM)
SOTU_mat_lsa <- lw_logtf(SOTU_mat_lsa) * gw_idf(SOTU_mat_lsa) # local - global weighting (akin to TFIDF)
SOTU_lsa <- lsa(SOTU_mat_lsa)
View(SOTU_lsa)
View(SOTU_lsa)
# Lecture example uses dims = 5
SOTU_lsa_5 <- lsa(SOTU_mat_lsa, 5)
# display generated LSA space
#?as.textmatrix
SOTU_lsa_5_mat <- t(as.textmatrix(SOTU_lsa_5))
SOTU_dfm@Dimnames$docs[9]
topfeatures(SOTU_dfm[9,])
# With 5 dims:
sort(SOTU_lsa_5_mat[9,], decreasing=T)[1:10]
# With auto (21) dims:
sort(t(as.textmatrix(SOTU_lsa))[9, ], decreasing = T)[1:10]
View(SOTU_lsa)
View(SOTU_lsa_5)
# Q: How are words related?
# associate(): a method to identify words that are most similar to other words using a LSA
#?associate
# uses cosine similarity between input term and other terms
SOTU_lsa_mat <- as.textmatrix(SOTU_lsa)
oil <- associate(SOTU_lsa_mat, "oil", "cosine", threshold = .7)
oil[1:10]
health <- associate(SOTU_lsa_mat, "health", "cosine", threshold = .7)
health[1:10]
# Keep this in mind when we do topic models!
# 3.1 Read in conservative and labour manifestos (from Recitation 6)
setwd("D:/Users/Lucia/Dropbox/NYU/Classes/TAD 2021/Lab Lectures/W6_03_11_21/cons_labour_manifestos")
files <- list.files(full.names=TRUE)
text <- lapply(files, readLines)
text <- unlist(lapply(text, function(x) paste(x, collapse = " ")))
# Name data
files <- unlist(files)
files <- gsub("./", "", files )
files <- gsub(".txt", "", files )
# Create metadata
year <- unlist(strsplit(files, "[^0-9]+"))
year <- year[year!=""]
party <- unlist(strsplit(files, "[^A-z]+"))
party <- party[party!="a" & party!="b"]
#create data frame
man_df <- data.frame(year = factor(as.numeric(year)),
party = factor(party),
text = text,
stringsAsFactors = FALSE)
# add text labels
man_df$text_label <- paste(man_df$party, man_df$year, sep = "_")
View(man_df)
lab_con_dfm <- dfm(man_df$text,
stem = T,
remove = stopwords("english"),
remove_punct = T
)
lab_con_dfm@Dimnames$docs <- man_df$text_label
View(lab_con_dfm)
manifestos_fish <- textmodel_wordfish(lab_con_dfm, c(1,24))
# visualize one-dimensional scaling
textplot_scale1d(manifestos_fish)
textplot_scale1d(manifestos_fish, groups = man_df$party)
library(quanteda)
# 3.2 fit wordfish
# Setting the index on parties (1st Con, 1st Lab)
manifestos_fish <- textmodel_wordfish(lab_con_dfm, c(1,24))
# 3.2 fit wordfish
# Setting the index on parties (1st Con, 1st Lab)
manifestos_fish <- quanteda::textmodel_wordfish(lab_con_dfm, c(1,24))
?textmodel_wordfish
??textmodel_wordfish
installed.packages("quanteda")
install.packages("quanteda")
install.packages("quanteda")
install.packages("quanteda")
install.packages("quanteda")
rm(list = ls())
# Loading packages
library(quanteda)
library(quanteda.corpora)
library(dplyr)
??textmodels
# one-dimensional text scaling method.
# unlike wordscores, it does not require reference texts
# 3.1 Read in conservative and labour manifestos (from Recitation 6)
setwd("D:/Users/Lucia/Dropbox/NYU/Classes/TAD 2021/Lab Lectures/W6_03_11_21/cons_labour_manifestos")
files <- list.files(full.names=TRUE)
text <- lapply(files, readLines)
text <- unlist(lapply(text, function(x) paste(x, collapse = " ")))
# Name data
files <- unlist(files)
files <- gsub("./", "", files )
files <- gsub(".txt", "", files )
# Create metadata
year <- unlist(strsplit(files, "[^0-9]+"))
year <- year[year!=""]
party <- unlist(strsplit(files, "[^A-z]+"))
party <- party[party!="a" & party!="b"]
#create data frame
man_df <- data.frame(year = factor(as.numeric(year)),
party = factor(party),
text = text,
stringsAsFactors = FALSE)
# add text labels
man_df$text_label <- paste(man_df$party, man_df$year, sep = "_")
lab_con_dfm <- dfm(man_df$text,
stem = T,
remove = stopwords("english"),
remove_punct = T
)
lab_con_dfm@Dimnames$docs <- man_df$text_label
# 3.2 fit wordfish
# Setting the index on parties (1st Con, 1st Lab)
manifestos_fish <- textmodel_wordfish(lab_con_dfm, c(1,24))
?quanteda::textmodel_wordshoal
?quanteda
################
## 3 WORDFISH ##
################
# one-dimensional text scaling method.
# unlike wordscores, it does not require reference texts
library(quanteda.textmodels)
# 3.2 fit wordfish
# Setting the index on parties (1st Con, 1st Lab)
manifestos_fish <- textmodel_wordfish(lab_con_dfm, c(1,24))
# visualize one-dimensional scaling
textplot_scale1d(manifestos_fish)
textplot_scale1d(manifestos_fish, groups = man_df$party)
# Plot of document positions
plot(year[1:23], manifestos_fish$theta[1:23]) # These are the conservative manifestos
points(year[24:46], manifestos_fish$theta[24:46], pch = 8) # These are the Labour manifestos
plot(as.factor(party), manifestos_fish$theta)
textplot_scale1d(manifestos_fish, groups = man_df$party)
plot(as.factor(party), manifestos_fish$theta)
# most important features--word fixed effects
words <- manifestos_fish$psi # values
names(words) <- manifestos_fish$features # the words
sort(words)[1:50]
sort(words, decreasing=T)[1:50]
s
# Guitar plot
weights <- manifestos_fish$beta
plot(weights, words)
# Set up workspace
rm(list = ls())
# Set up workspace
rm(list = ls())
# Loading packages
#install.packages("lsa")
#install.packages("factoextra")
library(quanteda)
library(quanteda.corpora)
library(dplyr)
library(factoextra) # makes it easy to work with PCA (great for visualization)
library(text2vec)
library(lsa)
#set directory
setwd("D:/Users/Lucia/Dropbox/NYU/Classes/TAD 2021/Lab Lectures/W9_04_01_21")
# 1.2 Example
data("data_corpus_sotu")
SOTU <- corpus_subset(data_corpus_sotu, Date > "1900-01-01")
SOTU_dfm <- dfm(SOTU,
stem = T,
remove_punct = T,
remove = stopwords("english")
)
SOTU_mat <- convert(SOTU_dfm, to = "matrix") # convert to matrix
# run pca
SOTU_pca <- prcomp(SOTU_mat, center = TRUE, scale = TRUE)
# visualize eigenvalues (scree plot: shows percentage of variance explained by each dimension)
fviz_eig(SOTU_pca, addlabels = TRUE)
# Loadings for each variable: columns contain the eigenvectors
SOTU_pca$rotation[1:10, 1:5]
dim(SOTU_pca$rotation)
# Can we interpret the dimensions?
pc_loadings <- SOTU_pca$rotation
# what do we expect this correlation to be?
cor(pc_loadings[,1], pc_loadings[,2])  # these should be orthogonal
N <- 10
pc1_loading <- tibble(token = rownames(pc_loadings), loading = as.vector(pc_loadings[,1])) %>% arrange(-loading)
pc1_loading$loading <- scale(pc1_loading$loading, center = TRUE)
pc1_loading <- rbind(top_n(pc1_loading, N, loading),top_n(pc1_loading, -N, loading))#lets get the top and bottom 10
pc1_loading <- transform(pc1_loading, token = factor(token, levels = unique(token)))
# plot top tokens according to absolute loading values
ggplot(pc1_loading, aes(token, loading)) +
geom_bar(stat = "identity", fill = ifelse(pc1_loading$loading <= 0, "grey20", "grey70")) +
coord_flip() +
xlab("Tokens") + ylab("Tokens with Top Loadings on PC1") +
scale_colour_grey(start = .3, end = .7) +
theme(panel.background = element_blank())
# Value of the rotated data: your "new", dimensionality reduced data
View(SOTU_pca$x)  # each observation
# function computes cosine similarity between query and all documents and returns N most similar
nearest_neighbors <- function(query, low_dim_space, N = 5, norm = "l2"){
cos_sim <- sim2(x = low_dim_space, y = low_dim_space[query, , drop = FALSE], method = "cosine", norm = norm)
nn <- cos_sim <- cos_sim[order(-cos_sim),]
return(names(nn)[2:(N + 1)])  # query is always the nearest neighbor hence dropped
}
# apply to document retrieval
nearest_neighbors(query = "Obama-2009", low_dim_space = SOTU_pca$x, N = 5, norm = "l2")
# Let's keep using the SOTU data from before
SOTU_mat_lsa <- convert(SOTU_dfm, to = "lsa") # convert to transposed matrix
#(so terms are rows and columns are documents = TDM)
SOTU_mat_lsa <- lw_logtf(SOTU_mat_lsa) * gw_idf(SOTU_mat_lsa) # local - global weighting (akin to TFIDF)
# 2.1 Create LSA weights using TDM
#lsa(myMatrix, dims = dimcalc_share(share = 0.8))
# share = fraction of the sum of the selected singular values over the sum of all singular values, default is 0.5
SOTU_lsa <- lsa(SOTU_mat_lsa)
# what do we expect this correlation to be?
cor(SOTU_lsa$tk[,1], SOTU_lsa$tk[,2])  # these should be orthogonal
View(SOTU_lsa)
# Lecture example uses dims = 5
SOTU_lsa_5 <- lsa(SOTU_mat_lsa, 5)
# display generated LSA space
#?as.textmatrix
SOTU_lsa_5_mat <- t(as.textmatrix(SOTU_lsa_5))
SOTU_dfm@Dimnames$docs[9]
topfeatures(SOTU_dfm[9,])
# With 5 dims:
sort(SOTU_lsa_5_mat[9,], decreasing=T)[1:10]
# uses cosine similarity between input term and other terms
SOTU_lsa_mat <- as.textmatrix(SOTU_lsa)
oil <- associate(SOTU_lsa_mat, "oil", "cosine", threshold = .7)
oil[1:10]
health <- associate(SOTU_lsa_mat, "health", "cosine", threshold = .7)
health[1:10]
# 3.1 Read in conservative and labour manifestos (from Recitation 6)
setwd("D:/Users/Lucia/Dropbox/NYU/Classes/TAD 2021/Lab Lectures/W6_03_11_21/cons_labour_manifestos")
files <- list.files(full.names=TRUE)
text <- lapply(files, readLines)
text <- unlist(lapply(text, function(x) paste(x, collapse = " ")))
# Name data
files <- unlist(files)
files <- gsub("./", "", files )
files <- gsub(".txt", "", files )
# Create metadata
year <- unlist(strsplit(files, "[^0-9]+"))
year <- year[year!=""]
party <- unlist(strsplit(files, "[^A-z]+"))
party <- party[party!="a" & party!="b"]
#create data frame
man_df <- data.frame(year = factor(as.numeric(year)),
party = factor(party),
text = text,
stringsAsFactors = FALSE)
# add text labels
man_df$text_label <- paste(man_df$party, man_df$year, sep = "_")
lab_con_dfm <- dfm(man_df$text,
stem = T,
remove = stopwords("english"),
remove_punct = T
)
lab_con_dfm@Dimnames$docs <- man_df$text_label
View(lab_con_dfm)
################
## 3 WORDFISH ##
################
# one-dimensional text scaling method.
# unlike wordscores, it does not require reference texts
library(quanteda.textmodels)
# 3.2 fit wordfish
# Setting the index on parties (1st Con, 1st Lab)
manifestos_fish <- textmodel_wordfish(lab_con_dfm, c(1,24))
# visualize one-dimensional scaling
textplot_scale1d(manifestos_fish)
textplot_scale1d(manifestos_fish, groups = man_df$party)
# Plot of document positions
plot(year[1:23], manifestos_fish$theta[1:23]) # These are the conservative manifestos
points(year[24:46], manifestos_fish$theta[24:46], pch = 8) # These are the Labour manifestos
# most important features--word fixed effects
words <- manifestos_fish$psi # values
names(words) <- manifestos_fish$features # the words
sort(words)[1:50]
sort(words, decreasing=T)[1:50]
# Guitar plot
weights <- manifestos_fish$beta
plot(weights, words)
