# keep only words that meet count threshold
vocab <- prune_vocabulary(vocab, term_count_min = MIN_COUNT)
# ================================
# create term co-occurrence matrix
# ================================
vectorizer <- vocab_vectorizer(vocab)
tcm <- create_tcm(it, vectorizer, skip_grams_window = WINDOW_SIZE,
skip_grams_window_context = "symmetric")
glove <- GlobalVectors$new(
rank = RANK,
x_max = 100,
lambda = 1e-5)
# ================================
# fit model
# ================================
word_vectors_main <- glove$fit_transform(tcm,
n_iter = ITERS,
convergence_tol = 1e-3,
n_check_convergence = 1L,
n_threads = RcppParallel::defaultNumThreads())
# ================================
# get output
# ================================
word_vectors_context <- glove$components
word_vectors <- word_vectors_main + t(word_vectors_context) # word vectors
# features?
head(rownames(word_vectors))
View(word_vectors)
View(word_vectors)
View(word_vectors_main)
# pretrained GLoVE embeddings
# download this from NYUClasses > Resources > Lab_Materials > Glove
# GloVe pretrained (https://nlp.stanford.edu/projects/glove/)
pretrained <- readRDS("glove.rds")
# function to compute nearest neighbors
nearest_neighbors <- function(cue, embeds, N = 5, norm = "l2"){
cos_sim <- sim2(x = embeds,
y = embeds[cue, , drop = FALSE],
method = "cosine", norm = norm)
nn <- cos_sim <- cos_sim[order(-cos_sim),]
return(names(nn)[2:(N + 1)])  # cue is always the nearest neighbor hence dropped
}
# e.g.
nearest_neighbors("state", word_vectors, N = 10, norm = "l2")
nearest_neighbors("state", pretrained, N = 10, norm = "l2")
nearest_neighbors("welfare", word_vectors, N = 10, norm = "l2")
nearest_neighbors("welfare", pretrained, N = 10, norm = "l2")
# choice parameters
WINDOW_SIZE <- 2
RANK <- 300
ITERS <- 10
MIN_COUNT <- 10
# load data
corp <- readRDS("news_data.rds")
text <- tolower(corp$headline)
# shuffle text
set.seed(42L)
text <- sample(text)
# ================================
# create vocab
# ================================
tokens <- space_tokenizer(text)
#rm(text)
it <- itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)
# keep only words that meet count threshold
vocab <- prune_vocabulary(vocab, term_count_min = MIN_COUNT)
# ================================
# create term co-occurrence matrix
# ================================
vectorizer <- vocab_vectorizer(vocab)
tcm <- create_tcm(it, vectorizer, skip_grams_window = WINDOW_SIZE,
skip_grams_window_context = "symmetric")
glove <- GlobalVectors$new(
rank = RANK,
x_max = 100,
lambda = 1e-5)
# ================================
# fit model
# ================================
word_vectors_main <- glove$fit_transform(tcm,
n_iter = ITERS,
convergence_tol = 1e-3,
n_check_convergence = 1L,
n_threads = RcppParallel::defaultNumThreads())
# ================================
# get output
# ================================
word_vectors_context <- glove$components
word_vectors <- word_vectors_main + t(word_vectors_context) # word vectors
# features?
head(rownames(word_vectors))
View(word_vectors)
# choice parameters
WINDOW_SIZE <- 6
RANK <- 300
ITERS <- 10
MIN_COUNT <- 10
# load data
corp <- readRDS("news_data.rds")
text <- tolower(corp$headline)
# shuffle text
set.seed(42L)
text <- sample(text)
# ================================
# create vocab
# ================================
tokens <- space_tokenizer(text)
#rm(text)
it <- itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)
# keep only words that meet count threshold
vocab <- prune_vocabulary(vocab, term_count_min = MIN_COUNT)
# ================================
# create term co-occurrence matrix
# ================================
vectorizer <- vocab_vectorizer(vocab)
tcm <- create_tcm(it, vectorizer, skip_grams_window = WINDOW_SIZE,
skip_grams_window_context = "symmetric")
glove <- GlobalVectors$new(
rank = RANK,
x_max = 100,
lambda = 1e-5)
# ================================
# fit model
# ================================
word_vectors_main <- glove$fit_transform(tcm,
n_iter = ITERS,
convergence_tol = 1e-3,
n_check_convergence = 1L,
n_threads = RcppParallel::defaultNumThreads())
# ================================
# get output
# ================================
word_vectors_context <- glove$components
word_vectors <- word_vectors_main + t(word_vectors_context) # word vectors
# features?
head(rownames(word_vectors))
View(word_vectors_context)
?word_vectors_context
?components
?word_vectors_main
# Prepare Data
mydata <- na.omit(mydata) # listwise deletion of missing
?mydata
??mydata
library(tidyverse)
?mydata
# Prepare Data
mydata <- na.omit(mydata) # listwise deletion of missing
# Prepare Data
mydata <- na.omit(mtcars) # listwise deletion of missing
mydata <- scale(mtcars) # standardize variables
# Prepare Data
mydata <- na.omit(iris) # listwise deletion of missing
mydata <- scale(iris) # standardize variables
# Prepare Data
mydata <- na.omit(USArrests) # listwise deletion of missing
mydata <- scale(USArrests) # standardize variables
View(mydata)
# Determine number of clusters
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(mydata,
centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
# K-Means Cluster Analysis
fit <- kmeans(mydata, 5) # 5 cluster solution
# get cluster means
aggregate(mydata,by=list(fit$cluster),FUN=mean)
# append cluster assignment
mydata <- data.frame(mydata, fit$cluster)
View(mydata)
# Ward Hierarchical Clustering
d <- dist(mydata, method = "euclidean") # distance matrix
fit <- hclust(d, method="ward")
plot(fit) # display dendogram
groups <- cutree(fit, k=5) # cut tree into 5 clusters
# draw dendogram with red borders around the 5 clusters
rect.hclust(fit, k=5, border="red")
# Ward Hierarchical Clustering with Bootstrapped p values
library(pvclust)
install.packages("pvclust")
# Ward Hierarchical Clustering with Bootstrapped p values
library(pvclust)
fit <- pvclust(mydata, method.hclust="ward",
method.dist="euclidean")
plot(fit) # dendogram with p values
# add rectangles around groups highly supported by the data
pvrect(fit, alpha=.95)
# Prepare Data
mydata <- na.omit(mtcars) # listwise deletion of missing
mydata <- scale(mycars) # standardize variables
mydata <- scale(mtcars) # standardize variables
mydata <- scale(mydata) # standardize variables
View(mydata)
# Determine number of clusters
wss <- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(mydata,
centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
# K-Means Cluster Analysis
fit <- kmeans(mydata, 5) # 5 cluster solution
# get cluster means
aggregate(mydata,by=list(fit$cluster),FUN=mean)
# append cluster assignment
mydata <- data.frame(mydata, fit$cluster)
install.packages("factoextra")
library(tidyverse)
library(factoextra)
# Load and scale the dataset
data("USArrests")
df <- scale(USArrests)
head(df)
# Distance matrix computation and visualization
# Correlation-based distance method
res.dist <- get_dist(df, method = "pearson")
head(round(as.matrix(res.dist), 2))[, 1:6]
# Visualize the dissimilarity matrix
fviz_dist(res.dist, lab_size = 8)
?get_dist
# Enhanced clustering analysis
# Load and scale the dataset
data("USArrests")
df <- scale(USArrests)
# Compute dissimilarity matrix
res.dist <- dist(df, method = "euclidean")
# Compute hierarchical clustering
res.hc <- hclust(res.dist, method = "ward.D2")
# Visualize
plot(res.hc, cex = 0.5)
# Enhanced k-means clustering
res.km <- eclust(df, "kmeans", nstart = 25)
# Gap statistic plot
fviz_gap_stat(res.km$gap_stat)
# Optimal number of clusters using gap statistics
res.km$nbclust
# Print result
res.km
# Enhanced hierarchical clustering
res.hc <- eclust(df, "hclust") # compute hclust
library(tidyverse)
library(factoextra)
# Load and scale the dataset
data("USArrests")
df <- scale(USArrests)
head(df)
df <- scale(USArrests)
head(df)
# Distance matrix computation and visualization
# Correlation-based distance method
res.dist <- get_dist(df, method = "pearson")
head(round(as.matrix(res.dist), 2))[, 1:6]
# Visualize the dissimilarity matrix
fviz_dist(res.dist, lab_size = 8)
# k-means clustering
eclust(df, "kmeans", k = 4)
# k-means clustering
test <- eclust(df, "kmeans", k = 4)
View(test)
# Enhanced k-means clustering
res.km <- eclust(df, "kmeans", nstart = 25)
# how do you want to cluster?
eclust(x, FUNcluster = "kmeans", hc_metric = "euclidean", ...)
# how do you want to cluster?
eclust(x, FUNcluster = "kmeans", hc_metric = "euclidean", ...)
library(tidyverse)
library(factoextra)
# Load and scale the dataset
data("USArrests")
df <- scale(USArrests)
head(df)
# Distance matrix computation and visualization
# Correlation-based distance method
res.dist <- get_dist(df, method = "pearson")
head(round(as.matrix(res.dist), 2))[, 1:6]
# Visualize the dissimilarity matrix
fviz_dist(res.dist, lab_size = 8)
# how do you want to cluster?
eclust(x, FUNcluster = "kmeans", hc_metric = "euclidean", ...)
# Enhanced k-means clustering
res.km <- eclust(df, "kmeans", nstart = 25)
# Print result
res.km
# Gap statistic plot
fviz_gap_stat(res.km$gap_stat)
# Enhanced k-means clustering
res.km <- eclust(df, "kmeans", nstart = 30)
# Print result
res.km
# Gap statistic plot
fviz_gap_stat(res.km$gap_stat)
# Enhanced k-means clustering
res.km <- eclust(df, "kmeans", nstart = 10)
# Print result
res.km
# Gap statistic plot
fviz_gap_stat(res.km$gap_stat)
?eclust
# Enhanced k-means clustering
res.km <- eclust(df, "kmeans", nstart = 1)
# Enhanced k-means clustering
res.km <- eclust(df, "kmeans", nstart = 2)
# Enhanced k-means clustering
res.km <- eclust(df, "kmeans", nstart = 45)
# Enhanced k-means clustering
res.km <- eclust(df, "kmeans", nstart = 3)
# Enhanced k-means clustering
res.km <- eclust(df, "kmeans", nstart = 10)
# Enhanced k-means clustering
res.km <- eclust(df, "kmeans", nstart = 15)
# Enhanced k-means clustering
res.km <- eclust(df, "kmeans", nstart = 9)
# Enhanced k-means clustering
res.km <- eclust(df, "kmeans", nstart = 10)
# Gap statistic plot
fviz_gap_stat(res.km$gap_stat)
?eclust
# Enhanced k-means clustering
res.km <- eclust(df, "kmeans", nstart = 100)
# Print result
res.km
# k-means clustering
eclust(df, "kmeans", k = 3)
# Enhanced k-means clustering
res.km <- eclust(df, "kmeans", nstart = 100)
# Print result
res.km
# Gap statistic plot
fviz_gap_stat(res.km$gap_stat)
# Enhanced hierarchical clustering
res.hc <- eclust(df, "hclust") # compute hclust
# Optimal number of clusters using gap statistics
res.km$nbclust
# Enhanced k-means clustering
res.km <- eclust(df, "kmeans", nstart = 100)
# Print result
res.km
# Gap statistic plot
fviz_gap_stat(res.km$gap_stat)
?fviz_gap_stat
# Enhanced hiearchical clustering analysis
# Load and scale the dataset
data("USArrests")
df <- scale(USArrests)
# Compute dissimilarity matrix
res.dist <- dist(df, method = "euclidean")
# Compute hierarchical clustering
res.hc <- hclust(res.dist, method = "ward.D2")
# Visualize
plot(res.hc, cex = 0.5)
# Enhanced k-means clustering
res.km <- eclust(df, "kmeans", nstart = 20)
# Print result
res.km
# Gap statistic plot
fviz_gap_stat(res.km$gap_stat)
# Optimal number of clusters using gap statistics
res.km$nbclust
# Enhanced hiearchical clustering analysis
# Load and scale the dataset
data("USArrests")
df <- scale(USArrests)
# Compute dissimilarity matrix
res.dist <- dist(df, method = "euclidean")
# Compute hierarchical clustering
res.hc <- hclust(res.dist, method = "ward.D2")
# Visualize
plot(res.hc, cex = 0.5)
# k-means clustering
eclust(df, "kmeans", k = 4)
# Load Packages
library(pacman)
p_load(magrittr, rvest, xml2)
# Set Working Directory
wd <- "/Users/noeljohnson_laptop/Dropbox/Mac/Desktop/scraping"
setwd(wd)
link <- "https://www.basketball-reference.com/teams/NYK/2019.html"
# Load Packages
library(pacman)
p_load(magrittr, rvest, xml2)
# Set Working Directory
wd <- "/Users/noeljohnson_laptop/Dropbox/Mac/Desktop/scraping"
setwd(wd)
##########################################
# Web Crawling (i.e. Saving HTML to your Computer)
##########################################
# Crawl a Single Page
link <- "https://www.basketball-reference.com/teams/NYK/2019.html"
link %>%
read_html() -> myHTML
View(myHTML)
myHTML %>%
write_html("knicks2019.html")
for (year in 2010:2019) {
paste0("https://www.basketball-reference.com/teams/NYK/",year,".html") %>%
read_html() %>%
write_html(paste0("knicks",year,".html"))
}
read_html("knicks2019.html") -> myHTML
for (file in files) {
file %>%
read_html() %>%
html_elements("#roster") %>%
html_table() %>%
as.data.frame() -> temp
temp$file <- file # Adds a file column so we know where each entry came from
roster[[length(roster) + 1]] <- temp # Adds the current table to the end of the roster list
}
myHTML %>%
html_elements("#roster") %>%
html_table() -> roster
roster %>%
as.data.frame() -> roster
View(roster)
list.files() -> files # This creates a list of the crawled html files from our working directory
# Load Packages
library(pacman)
p_load(magrittr, rvest, xml2)
# Set Working Directory
wd <- "/Users/noeljohnson_laptop/Dropbox/Mac/Desktop/scraping/html_files"
setwd(wd)
##########################################
# Web Crawling (i.e. Saving HTML to your Computer)
##########################################
# Crawl a Single Page
link <- "https://www.basketball-reference.com/teams/NYK/2019.html"
link %>%
read_html() -> myHTML
myHTML %>%
write_html("knicks2019.html")
# Crawl Multiple Pages
for (year in 2010:2019) {
paste0("https://www.basketball-reference.com/teams/NYK/",year,".html") %>%
read_html() %>%
write_html(paste0("knicks",year,".html"))
}
##########################################
# Web Scraping (i.e. Extracting Information from the Saved HTML) Tables
##########################################
# Extract a Single Table
read_html("knicks2019.html") -> myHTML
myHTML %>%
html_elements("#roster") %>%
html_table() -> roster
roster %>%
as.data.frame() -> roster
# Extract Multiple Tables
list.files() -> files # This creates a list of the crawled html files from our working directory
roster <- list() # Creates a list for storing our tables, making combining them later easier
for (file in files) {
file %>%
read_html() %>%
html_elements("#roster") %>%
html_table() %>%
as.data.frame() -> temp
temp$file <- file # Adds a file column so we know where each entry came from
roster[[length(roster) + 1]] <- temp # Adds the current table to the end of the roster list
}
View(temp)
View(roster)
?do.call
rosterFilled <- do.call(rbind, roster)
View(rosterFilled)
read_html("knicks2019.html") -> myHTML
myHTML %>%
html_elements("#roster") %>%
html_table() %>%
as.data.frame() -> knicks
myHTML %>%
html_elements("#roster") %>%
html_elements("a") %>%
html_attr("href") -> links # Here we use html_attr instead of html_table
head(links)
View(rosterFilled)
links <- links[grepl("players",links)]
head(links)
# We almost have what we want, but we need to add the rest of the hyperlink first
links <- paste0("https://www.basketball-reference.com", links)
# Now we have our player links and can add them to the data frame!
knicks$playerLinks <- links
View(knicks)
"https://www.basketball-reference.com/teams/NYK/3019.html" %>%
read_html()
tryCatch({
"https://www.basketball-reference.com/teams/NYK/3019.html" %>%
read_html()
}, error = function(e){
print("404 error ... try again")
})
?tryCatch
??tryCatch
tryCatch({
"https://www.basketball-reference.com/teams/NYK/2019.html" %>%
read_html()
}, error = function(e){
print("404 error ... try again")
})
tryCatch({
"https://www.basketball-reference.com/teams/NYK/2019.html" %>%
read_html()
}, error = function(e){
print("404 error ... try again")
})
8539*12
(8539*12)*1.17
0.49*1.77
.87/.95
0.95*0.035
0.95*0.35
0.2*.8
